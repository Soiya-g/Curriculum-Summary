## 离散单符号信源
---
离散单符号信源输出==单符号消息==，各消息==互不相容（独立）==。
若用 $X$ 表示信源，则信源空间为
$$
\begin{bmatrix}
X\\p(x)
\end{bmatrix} 
=
\begin{bmatrix}
a_1 & a_2 & \cdots &a_n\\p(a_1) & p(a_2) & \cdots & p(a_n)
\end{bmatrix} 
$$
且 $\sum_{i=1}^n p(a_i) = 1$


## 离散无记忆序列信源
---
离散无记忆序列信源输出==多符号消息==，各符号互相==独立==。
若用 $X$ 表示信源，则信源空间为
$$
\begin{bmatrix}
X\\p(x)
\end{bmatrix} 
=
\begin{bmatrix}
s_1 & s_2 & \cdots & s_n\\p(s_1) & p(s_2) & \cdots & p(s_n)
\end{bmatrix} 
$$
	其中的 $X$ 为 $(X_1,X_2,X_3,\cdots,X_n)$，$X_k \in A=[a_1,a_2,a_3,\dots,a_q]$，所以 $X$ 的取值为 $q^N$ 个。


## 离散有记忆序列信源
---
离散无记忆序列信源输出==多符号消息==，各符号互相==关联==。
典型的有马尔可夫信源（马氏链信源），即 $x_{m+1}$  只跟 $x_m$ 有关，联合概率表示为
$$
\begin{eqnarray}
p(x_1x_2x_3\cdots x_n) &=& p(x_1)p(x_2|x_1)p(x_3|x_1x_2)\cdots p(x_n|x_1\cdots x_n) \\&=& p(x_1)\prod_{i=1}^{n-1}p(x_{i+1}|x_i)
\\&=& p_1p^{n-1}_{ji}
\\&\cong& p_{ji}^n
\end{eqnarray}
$$

其中 $p_{ji}^n$ 为马氏链的状态转移矩阵。

## 连续信源
---
连续信源在时间与取值上都==连续==，往往采用两类方式进行分析：
1. 将连续信源==离散化==为==随机序列信源==，再用离散的方法进行分析；
2. 直接分析连续信源。
>
>注意：只有满足==限时（T）==与==限频（F）==的连续模拟信源才能离散化为随机序列信源。

三种常见的的离散化方法：
1. 傅里叶级数展开（周期连续信源）
![[连续信号傅里叶级数展开.jpg]]
2. 取样函数展开
![[连续信号取样函数展开.jpg]]
![[连续信号取样函数展开_1.jpg]]
3. K-L展开
![[K-L展开.jpg]]
![[K-L展开_1.jpg]]



## 离散单符号信源的熵与互信息
---
### 自信息量

==信息量 $I(x)$ ==：表示事件发生前的==不确定度==；发生后，表示消息 $x$ 当中的==信息量==，一般接收的信息量==小于== $I(x)$，$I(x)$ 要满足：
1. $p(a_i)=1$ 时，$I[a_i]=0$；
2. $p(a_i)=0$ 时，$I[a_i]=\infty$；
3. 两个独立事件的联合信息量为==各自信息量之和==。
即可得公式 $I(x=a_i) = -\log_a{p(a_i)} =\log_a\frac{1}{p(a_i)}$。
==单位（以a为底）==：$a = 2 (bit)$、$a = e(nat)$、$a = 10(hart)$
==联合信息量==：联合概率空间 $[XY,p(xy)]^T$ 的联合自信息量$I(xy) = -\log_a{p(xy)}$
==条件信息量==：联合概率空间 $[XY,p(xy)]^T$ 的事件 $x \in X$ 在条件 $y\in Y$ 下的条件自信息量$I(x|y) = -\log_a{p(x|y)}$
==三者关系==：$I(xy)=I(x)+I(y|x)=I(y)+I(x|y)$
==信息的链式规则==：$I(x_1x_2\cdots x_n)=I(x_1)+I(x_2|x_1)+ \cdots +I(x_n|x_1x_2\cdots x_{n-1})$

---
### 信息熵

==信息熵==：离散概率空间为 $[X,p(x)]^T$ 的信源，随机变量 $I(x)$ 的==数学期望==，表示为
$H(X) = E(I(x)) = -\sum_{x\in X}p(x)\log p(x)$（比特/符号）
特定信源信息熵==固定==。
==条件信息熵（给定条件 ）==：在概率空间 $[X,p(x)]^T$ 下，随机变量 $I(x|y)$ 在集合 $X$ 上的期望在给定 $y \in Y$ 条件下 $X$ 的条件熵：
$H(X|y) = E_x[I(x|y)] = \sum_{x \in X}p(x|y)I(x|y) =-\sum_{x \in X} p(x|y)\log{p(x|y)}$
==条件信息熵（条件空间）==：在概率空间 $[Y,p(y)]^T$ 下，随机变量 $H(X|y)$ 的期望为 $H(X|Y)$ ，$X$ 的条件熵：
$H(X|Y) = E_y[H(X|y)]=\sum_{y\in Y}p(y)H(X|y) = -\sum_{x\in X}\sum_{y\in Y}p(y)p(x|y)\log{p(x|y)}$
==联合熵==：表示一对符号的平均不确定度，在联合概率空间 $[XY,p(xy)]^T$ 上的随机变量 $I(xy)$ 的数学期望为集合 $X$ 与集合 $Y$ 的联合熵：
$H(XY) =E[I(xy)] = \sum_{x\in X}\sum_{y\in Y} p(xy)I(xy) = -\sum_{x\in X}\sum_{y\in Y}p(xy)\log{p(xy)}$
==关系==：
$H(XY) = H(X) + H(Y|X) = H(Y) + H(X|Y)$
当集合 $X$、$Y$ ==相互独立==时
$H(Y|X) = H(Y)$
$H(X|Y) = H(X)$
$H(XY) = H(X) + H(Y)$
==熵的链式规则==：
$H(X_1X_2X_3\cdots X_n) = H(X_1) + H(X_2|X_1) + H(X_3|X_1X_2) + \cdots + H(X_n|X_1X_2X_3\cdots X_{n-1})$

---
### 信息熵的基本特性

1. ==非负性==：${\color{blue} }H(x)\ge 0$
2. ==对称性 ==：变量==顺序互换==，熵值==不变==。
3. ==确定性==：$H(1,0) = H(1,0,0) = H(1,0,0,0) = 0$，即确定性信息的熵确定为 $0$。
4. ==可加性==：因为 $\sum_{j=1}^{m_i}p_{ji}=1,\sum_{i=1}^{n}=1,\sum_{i=1}^nm_i=M$，有
$$
\begin{eqnarray}
&H(p_1p_{11},p_1p_{21},\cdots ,p_1p_{m_{i1}1}, p_2p_{12},p_2p_{22},\cdots ,p_2p_{m_{i2}2}, p_np_{1n},p_np_{2n},\cdots ,p_np_{m_{n}n})\\
=& H(p_1,p_2,\cdots p_n) + \sum p_iH_{m_i}(p_{1i},p_{2i},\cdots,p_{m_ii})
\end{eqnarray}
$$
5. ==香农辅助定理（极值定理）==：信源两种分布 $\sum_i^np_i = \sum_i^nq_i = 1$
$$H(p_1,p_2,\cdots,p_n)=-\sum_{i=1}^np_i\log{p_i}\le\sum_{i=1}^np_i\log{q_i}$$
当且仅当 $p_i = q_i$ 时等号成立。
6. ==最大熵==：$H(X)\le H(\frac{1}{M},\cdots,\frac{1}{M})=\log{M}$
7. ==上凸==

---

### 互信息量

信源传输信息量给信宿时，信宿接收到的信源的信息量部分（信宿接受后对信源消除的不确定度）就是两者的互信息量，即给定联合概率空间 $[XY,p(xy)]^T$ 在出现 $y\in Y$ 后提供的有关时间 $x\in X$ 的信息量 $I(x;y)$ 为
$$
I(x;y)=\log_a{\frac{1}{\frac{p(x)}{p(x|y)}}}=\log_a{\frac{1}{p(x)}}-\log_a{\frac{1}{p(x|y)}}=\log_a{\frac{p(x|y)}{p(x)}}=\log_a{p(x|y)-\log_a{p(x)}}
$$
类似
$$
I(y;x)=\log_a{\frac{p(y|x)}{p(y)}}=\log_a{p(y|x)-\log_a{p(y)}}
$$
==对称性==：
$$
I(x;y)=I(y;x)
$$
==条件互信息量==：条件互信息量在三维概率空间及以上的概率空间分布上定义，给定三维概率分布 $[XYZ,p(xyz)]^T$ 在 $Z$ 事件给定的条件下，事件 $x\in X$ 与 $y \in Y$ 的条件互信息量为：
$$ 
I(x;y|z) = \log(\frac{p(x|yz)}{p(x|z)})
$$
==联合互信息量==：给定三维概率分布 $[XYZ,p(xyz)]^T$ ，事件 $x\in X$ 与 $yz \in YZ$ 的联合互信息量为：
$$
I(x;yz) = \log(\frac{p(x|yz)}{p(x)})
$$
==关系==：
$$
I(x;yz) = I(x;z) + I(x;y|z)
$$
对称得
$$
I(x;yz) = I(y;x) + I(z;x|y) \quad 少用
$$
==单条件平均互信息量==：互信息量的数学期望，给定联合概率空间 $[XY,p(xy)]^T$ 在给定的 $y\in Y$ 的条件下，互信息量 $I(x;y)$ 的数学期望：
$$
I(X;y) = E_X[I(x;y)]=\sum_{x\in X}p(x|y)\log{\frac{p(x|y)}{p(x)}}
$$
同理，给定 $x\in X$：
$$
I(x;Y) = E_Y[I(x;y)]=\sum_{y\in Y}p(y|x)\log{\frac{p(y|x)}{p(y)}}
$$
==平均互信息量==：给定联合概率空间 $[XY,p(xy)]^T$ ，平均互信息量，在整个 $Y$ 上加权平均为平均互信息量 $I(X;Y)$：
1. 
$$
I(X;Y) = E_Y[I(X;y)]=\sum_{y\in Y}p(y)I(X;y)=\sum_{x\in X}\sum_{y\in Y} p(y)p(x|y)\log{\frac{p(x|y)}{p(x)}}
$$
2. $$
I(X;Y) = E_X[I(x;Y)]=\sum_{x\in X}p(x)I(x;Y)=\sum_{x\in X}\sum_{y\in Y} p(x)p(y|x)\log{\frac{p(y|x)}{p(y)}}
$$
3. $$
I(X;Y) = E_{XY}[I(x;y)]=\sum_{x\in X}\sum_{y\in Y} p(xy)\log{\frac{p(xy)}{p(x)p(y)}}
$$
==疑义度与损失熵==：由平均互信息量 1 式，有 $I(X;Y) = H(X) - H(X|Y)$，将 $H(X|Y)$ 称为疑义度或损失熵，即 $Y$ 接收到信息后，对于 $X$ 残余的不确定度。
==噪声熵==：由平均互信息量 2 式，有 $I(X;Y) = H(Y) - H(Y|X)$，将 $H(Y|X)$ 称为损失熵，即 $Y$ 接收到的不属于 $X$ 的信息量。
==后验不确定度==：由平均互信息量 3 式，有 $I(X;Y) = H(X) + H(Y) - H(XY)$，将 $H(XY)$ 称为后验不确定度，即在 $Y$ 接收信息后，整个系统残余的不确定度
### 平均互信息量性质
1. ==非负性==：没有互信息为 0；
2. ==互易性==：$I(X;Y) = I(Y;X)$，互信息是相互的；
3. ==可以条件熵与联合信息熵表示==：
$$
I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(XY)
$$
4. ==极值==：$I(X;Y)\leq H(X)$，$I(X;Y)\leq H(Y)$
5. ==凸性==：可以将 $I(X;Y)$ 描述为 $p(x)$ （信源特性）与 $p(y|x)$ （信道特性）的函数：
	- $p(y|x)$ 给定，$I(X;Y)$ 为上凸函数，关于 $p(x)$ 有最优信源特性，即等概分布；
	- $p(x)$ 给定，$I(X;Y)$ 为下凸函数，关于 $p(x)$ 有最差信道。
6. ==不增性==：多次传输后的互信息量不会增加。
## 离散序列信源的熵与互信息
==无记忆序列信源熵==：给定长度为 $L$ 随机序列 $X^L = \{X_1,X_2,\cdots,X_L\}$ ，联合概率为每个符号概率的乘积 $p(X^L) = \prod_{i=1}^Lp(X_l)$，所以无记忆序列信源熵：
$$
H(X) = \sum_{l=1}^LH(X_l)
$$
==离散平稳信源==：若序列满足
1. $X_i$ 取值有限；
2. 随机序列平稳，即与时间起点无关，任意两个时刻发出符号的概率相同，$p(X_i) =p(X_{i+h})$
称为离散平稳信源，结论
- $H(X_L|X_1^{L-1})$ 是 $L$ 的单调非增函数；
- $H_L(X)\ge H(X_L|X_1^{L-1})$；
- $H_L(X)$ 为 $L$ 的单调非增函数；
- 当 $L \rightarrow \infty$ 时，$H_{\infty}(X)=\lim_{L\rightarrow \infty }H(X_L|X_1X_2\cdots X_{L-1})$ ，称为极限熵，也称为熵率；
- $H_0(X)\ge H_1(X)\ge H_2(X)\ge \cdots \ge H_{\infty}(X)$，其中 $H_0(X)$ 为等概单符号信源的熵，$H_1(X)$ 为一般单符号信源的熵。
==离散平稳无记忆信源信息熵==：$H(X^L) = LH(X_1)$
==消息熵==：每个符号平均的熵，$H_L(X^L)=\frac{1}{L}H(X)$ 
### 马尔可夫信源
==马尔可夫信源==：信源的符号序列只和前 $m(m<L)$ 个时刻有关，与前面的其他符号无关，称其为 $m$ 阶马尔可夫信源，即 $p(x_i|x_{i-1}x_{i-2}x_{i-3}\cdots x_{i-m})$。
==马尔克夫链==：$\{X_n,n\in N\}$ 为随机序列，事件参数 $N = \{0,1,2\cdots\}$ 状态空间为 $S = \{S_1,S_2,\cdots\}$ 对所有的 $n\in N$ ，有 $p(X_n=S_{i_n}|X_{n-1}=S_{i_{n-1}},X_{n-2}=S_{i_{n-2}}\cdots)=p(X_n=S_{i_n}|X_{n-1}=S_{i_{n-1}})$ ，称其为马尔可夫链。
==转移概率==：从 $m$ 时刻的 $S_i$ 状态经过 $k$ 步转移到 $n$ 时刻的 $S_j$ 状态，定义为 $p_{ij}^{(k)}(X_{n}=S_j|X_{m}=S_i)$ ，简记为 $p_{ij}^{(k)}(mn)$ ，更一般记为 $p_{ij}^{(k)}(m)=p_{ij}^{(k)}(X_{m+k}=S_j|X_{m}=S_i)$ ，所有的转移概率结合可以表示为==转移概率矩阵==，显然 $\sum_{j\in S}p_{ij}^{(k)}(m) = 1$，即矩阵行之和为 $1$。
==初始分布==：转移概率不包含第 $0$ 次中的 $X_0=S_i$，所以需要引入初始分布来完整描述马尔可夫特性。
==齐次马尔可夫链==：转移状态与当前次序没有关系，即与 $m$ 无关。
==莫格罗夫方程==：$p^{(m)}p^{(r)}=p^{(m+r)}$ 或 $p_{ij}^{m+r}=\sum_{k\in S}p_{ik}^{(m)}p_{kj}^{(r)}$ 。
==遍历性==：无论从哪一个状态出发，当转移步数 $n$ 足够大时，到达 $S_j$ 的概率都一样，此时的分布称为==平稳分布==，通过找到 $n$ 步（$n\in N$，为正整数）转移矩阵概率是否==全大于零==，来判断是否有平稳分布 $\omega$ ，使用 $\omega P=\omega$ 来求平稳分布。
==马尔可夫信源极限熵==：为 $m$ 阶马尔可夫信源提供的平均信息量 $H_{\infty}$，当时间足够长，信源为平稳分布，而其又只与前 $m$ 个符号有关所以有
$$
H_{\infty}=\lim_{N\rightarrow \infty}H(X_N|X_1X_2X_3\cdots X_{N-1})=H(X_{m+1}|X_1X_2X_3\cdots X_{m})=H_{m+1}
$$
即 $m$ 阶马尔可夫信源的极限熵等于 $m$ 阶条件熵，表示为 $H_{m+1}$，使用平稳分布进行计算。
$$
H_{\infty}=H_{m+1}=\sum_{S_j}p(S_j)H(X|S_j)
$$
其中 $p(S_j)$ 为每个状态的==平稳分布==，$H(X|S_j)$ 为信源处于 $S_j$ 状态发出一个消息符号的不确定性，即为 $S_j$ 状态下得到 $X$ 的信息熵。

### 相关性与冗余度
$H_{\infty}$ 非常难求，而用 $m$ 阶马尔可夫信源的 $H_{m+1}$ 来近似大部分情况下都可以。对于离散平稳信源，输出每个符号的平均信息量用 $H_{\infty}$ 表示。
==相关性==：符号间相关性==越强==， $H_{\infty}$ ==越小==，在符号彼此不存在==依赖关系==且为==等概分布==时，$H_{\infty}$ 等于最大熵 $H_0$。
==冗余度==：信源在实际发送信息时包含的多余信息，取决于信息==相关性==与信源符号==分布不均匀==。对于一般平稳信源使用传送 $H_m$ 的方法传递 $H_{\infty}$ 的信息，就会存在多余信息，定义==信息效率==
$$\eta=\frac{H_{\infty}}{H_{m}}$$
表示不确定性的程度，进而定义==冗余度==
$$
\gamma = 1-\eta = 1- \frac{H_{\infty}}{H_{m}}
$$
